Automation Roles and Responsibilities

I closely worked in an Agile environment, where I actively participated in all Agile ceremonies like sprint planning, daily stand-ups, and retrospectives.

I made sure to understand the project requirements, user stories clearly and manual test cases so that I could plan and prioritize my testing activities effectively within each sprint and to get clarity over functionality.

I was actively involved in the script development process by analyzing manual test cases and functional requirements. Based on that, I designed and automated test scripts to ensure coverage of key business scenarios and functionality.

I also maintained a Requirement Traceability Matrix (RTM) to ensure that all the test cases were properly mapped to the corresponding requirements. This helped in tracking test case coverage and ensured that no requirement was left untested.

I was  involved in creating and maintaining POM (Page Object Model) classes to store web elements in a centralized object repository, which helped improve code reusability and maintainability in our automation framework.

I ensured all test scripts were executed and validated locally for accuracy and correctness before adding them to the shared automation framework for further execution.

If a valid defect was identified during local execution, I analyzed the issue thoroughly and logged it in JIRA with complete details including steps to reproduce, screenshots, and logs. As a Test Engineer, my goal was to ensure that every defect was clearly documented and visible to the entire team for quick understanding and resolution

I was involved in retesting for resolved defects and executing regression test suites to ensure the overall stability and functionality of the application after code changes.

I have used Maven commands to execute test suites as part of the automated build and test process, especially during regression cycles and before release, to ensure application stability and catch defects early.

I used GitHub to manage version control for all our automation scripts. I regularly committed my changes, worked with branches for new feature testing, and merged code after reviewing. It helped us collaborate as a team without overwriting each other's work.

I was involved in debugging the failed test scripts by checking logs, screenshots, and error messages. I fixed issues in scripts like broken locators or synchronizations.
I also used breakpoints and step-by-step execution in the IDE to trace the flow of the test script,  I also collaborated with developers to confirm whether the issue was due to a script error or an actual defect in the application.


What are the automation challenges you faced  in your previous project.

In my 3.6 years of experience, Iâ€™ve faced several challenges during automation testing that helped me grow technically. One of the common issues was when the requirements kept changing. This made it hard to plan test cases in advance. I also faced bugs that didnâ€™t always happenâ€”like they appeared once and then never again. These were hard to reproduce and took extra time to investigate and raised.

One technical challenge I faced was synchronization issues, where elements took time to load and caused test failures. To handle this, I implemented implicit waits for global delays, explicit waits for specific conditions, and custom waits for dynamic content. This helped improve the stability and reliability of our automation scripts.


At one point, we faced tool version compatibility issues â€” for example, after upgrading the browser or WebDriver, some scripts started failing due to mismatches between the browser version, Selenium WebDriver, and dependencies. To fix this, we ensured all tools and libraries were compatible, updated the required versions in the pom.xml, and used WebDriverManager to handle driver versions dynamically. This helped restore stability and avoided manual driver setup issues.

One of the challenges I faced was dealing with dynamic elements whose attributes like IDs or classes changed frequently, leading to script failures. To overcome this, I avoided direct static locators and instead used stable strategies like relative XPath and CSS selectors. I also applied advanced XPath axes such as ancestor, descendant, parent, child, and sibling to locate elements based on their relationship with nearby stable elements. This approach helped me build more robust and maintainable test scripts, especially for pages with complex or dynamic DOM structures.

introduction
project explanation
roles and responsibility 
challenges
framework




1. Object Repository â€“ Page Object Model (POM)
We followed the Page Object Model (POM) design pattern, where each page of the application (like Login Page, Home Page, Cart Page) had a corresponding Java class. These POM classes contained web element locators and methods to interact with those elements. This helped us keep the code clean, reusable, and easier to maintain when the UI changed.

ðŸ”¹ 2. Generic Utilities
Under a separate package (like com.purplle.utilities), we created multiple reusable utility classes to support the framework:

JavaUtility â€“
This included general Java functions like getting the current date/time, generating random numbers, or converting data types.

ExcelUtility â€“
Used Apache POI to read data from Excel sheets for data-driven testing. It supported methods to fetch cell data, row/column count, etc.

FileUtility â€“
This handled reading data from property files, like getting browser name, application URL, and timeout values from config.properties.

WebDriverUtility â€“
Contained methods to handle WebDriver-related tasks such as switching between windows, handling alerts, frames, dropdowns, mouse actions, scrolling, taking screenshots, and waits (explicit/implicit/custom).

ListenerUtility â€“
Implemented the TestNG ITestListener interface to listen to test events. On failure, it captured screenshots, logged status, and integrated with ExtentReports automatically.

BaseClass â€“
Served as a parent class for all test classes. It included @BeforeClass, @BeforeMethod, @AfterMethod, and @AfterClass methods for setting up and tearing down browser instances, initializing page classes, and reading configuration before each test.






















